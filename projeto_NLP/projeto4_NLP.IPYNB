{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "projeto4_NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "rxLYuJ3IjFdk",
        "xVos9YWln6m7",
        "r8cDe6KMX1aa"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxLYuJ3IjFdk"
      },
      "source": [
        "# **Descrição do projeto do Módulo de NLP**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0r5BTBhKibMm"
      },
      "source": [
        "\r\n",
        "No projeto do módulo de NLP vamos tentar resolver um problema de classificação de textos. Você escolherá um de três datasets cujo objetivo é classificar em um conjunto de classes. Você deverá utilizar três metodologias ensinadas em nossas aulas para poder classificar esses textos. A seguir, mais detalhes do projeto para que você possa resolver.\r\n",
        "\r\n",
        "\r\n",
        "Os produtos finais deste projeto será um Notebook Python. As seções seguintes detalham como você deve proceder para gerar o código e a última seção deve especificar a estrutura que seu notebook deve seguir.\r\n",
        "\r\n",
        "## 1. Dados\r\n",
        "Você poderá escolher três tarefas para resolver no projeto. A seguir existe a breve descrição de cada tarefa e um link para onde você poderá baixar os dados. \r\n",
        "\r\n",
        "O corpus UTL é um corpus com críticas de filmes e apps coletadas automaticamente de sites. As classes são: positiva ou negativa. Assim o usuário pode ter gostado ou não gostado do produto. Referência: https://github.com/RogerFig/UTLCorpus \r\n",
        "O corpus UOL AES-PT é um corpus de redações no estilo do ENEM. Cada redação possui um tópico e um conjunto de redações relacionadas. Nesse corpus, existem vários tópicos e suas respectivas redações. O objetivo é predizer a nota final de cada redação de acordo com o grupo de notas 0, 200, 400, 600, 800 e 1000. Para mais informações e download dos dados, acesse o link: https://github.com/evelinamorim/aes-pt .\r\n",
        "O corpus TweetSentBr é um corpus em português de tweets. Cada tweet está rotulado com uma das classes: positivo, negativo e neutro. Para mais informações e download do corpus, acesse o link https://bitbucket.org/HBrum/tweetsentbr/src/master/ .\r\n",
        "\r\n",
        "## 2. Representação\r\n",
        "Vimos durante a nossa aula diversas forma de representar um documento de texto. Você vai usar cada uma dessas representações e compará-las. A seguir temos a listagem das representações que devem ser usadas para representar seu texto.\r\n",
        "Representação TF-IDF. Você pode usar tanto o gensim quanto o scikit para montar esta representação, mas lembre-se que é importante fazer o pré-processamento dos textos.\r\n",
        "Representação com o word2vec. O modelo poderá ser o apresentado na aula 03 ou algum outro modelo pré-treinado como os existentes no repositório http://nilc.icmc.usp.br/nilc/index.php/repositorio-de-word-embeddings-do-nilc . Neste caso, cada documento deverá ser representado pelo vetor que resultar da média dos vetores de todas as palavras que o compõem. Em outras palavras, se D é composto pelas palavras w1, w2, …, wn, e seus vetores embeddings são v1, v2, …, vn, então a representação do documento de D será v = (v1 + v2 + … + vn) / n. \r\n",
        "Extração de features do texto. Você deve pensar em ao menos 10 features para extrair do documento e que o possam representar. Aqui vão algumas sugestões: número de palavras, número de verbos, número de conjunções, número de palavras negativas, número de palavras fora do vocabulário, quantidades de entidades do tipo PESSOA, quantidade de entidades do tipo LOCAL, etc.\r\n",
        "\r\n",
        "Lembrando que você deve dividir seu conjunto em treino e teste. No TF-IDF, você só pode aplicar o método fit no conjunto de treino. Uma sugestão é dividir 80% do conjunto de dados para treino e 20% para teste. Essa divisão é aleatória, mas você pode usar o método train_test_split para essa divisão. O exemplo a seguir mostra como usar esse método:\r\n",
        "\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "\r\n",
        " X_train, X_test, y_train, y_test = train_test_split(\r\n",
        "...    X, y, test_size=0.20, random_state=42)\r\n",
        "\r\n",
        "## 3. Visualização dos dados\r\n",
        "\r\n",
        "Também vimos que embora o nosso texto apresente dimensionalidade maior que 2D, é possível visualizar em apenas duas dimensões usando técnicas de redução de dimensionalidade. Vimos duas técnicas de redução de dimensionalidade, o PCA e o t-SNE. Assim, pede-se que você utilize as duas técnicas para gerar uma visualização dos seus dados e considere as classes para colorir as instâncias. \r\n",
        "\r\n",
        "Sugere-se utilizar a biblioteca yellowbrick para gerar as visualizações, devido sua simplicidade. Mas caso tenha interesse em gerar visualizações mais interativas e mais bonitas, você pode utilizar a biblioteca seaborn. Para uma galeria dos gráficos que o seaborn é capaz de fazer, acesse o link https://seaborn.pydata.org/examples/index.html . Apenas acrescentando em seu código import seaborn as sns; sns.set() , também é possível deixar o gráfico com cores mais bonitas. Todas essas bibliotecas precisam do matplotlib, que já está importado no exemplo da aula.\r\n",
        "\r\n",
        "Aqui você deve fazer a visualização apenas do seu conjunto de treino.\r\n",
        "## 4. Classificadores\r\n",
        "\r\n",
        "Escolha dois classificadores que você possua mais familiaridade no scikit-learn para poder classificar os seus dados. Você deve executar cada um dos classificadores nas três representações escolhidas. \r\n",
        "\r\n",
        "Você pode usar o k-nn como um dos métodos. Outros métodos estão disponíveis no scikit, como por exemplo o SVM e o RandomForest.\r\n",
        "\r\n",
        "## 5. Métricas de avaliação\r\n",
        "\r\n",
        "Para os corpus TweetSentBR e UTL, pede-se que se use a matriz de confusão, a precisão, o recall e o f-1 para reportar a acurácia dos seus classificadores. No caso do corpus UOL AES-PT pede-se que se use o erro médio apenas.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0myAsoSHid2I"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYYabv8fjWif"
      },
      "source": [
        "# Estrutura"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynv0u9vojj9D"
      },
      "source": [
        "Estrutura do Notebook\r\n",
        "\r\n",
        "O seu notebook deve ser dividido por seções que possuam  uma célula do tipo Markdown. Nesta célula deve ter o título da seção antecedida por um marcador do tipo #. O título de cada seção deverá ser como a lista abaixo. Além do título, é possível que a seção demande a descrição de resultados ou outro tipo de texto. Nestes casos, coloque o texto junto à célula do título. Se houver código solicitado para a seção, então as células restantes devem ser de código solicitado.  \r\n",
        "O relatório deve ser organizado nas seguintes seções:\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "1.   Tarefa e Dados\r\n",
        "\r\n",
        "2.   Classificadores\r\n",
        "\r\n",
        "3.   Resultados\r\n",
        "\r\n",
        "4.   Resultados\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVos9YWln6m7"
      },
      "source": [
        "## 1 - Tarefa e Dados: \r\n",
        "Descreva a tarefa escolhida e os dados. Escreva código que leia os dados e calcule e imprima quantas instâncias os dados têm. Também, seu código deve calcular a média de tokens por instância, isto é, quantos tokens, na média cada documento do seu conjunto de dados possui. Imprima esse único número.\r\n",
        "\r\n",
        "Você poderá escolher três tarefas para resolver no projeto. A seguir existe a breve descrição de cada tarefa e um link para onde você poderá baixar os dados.\r\n",
        "\r\n",
        "O corpus UTL é um corpus com críticas de filmes e apps coletadas automaticamente de sites. As classes são: positiva ou negativa. Assim o usuário pode ter gostado ou não gostado do produto. Referência: https://github.com/RogerFig/UTLCorpus.\r\n",
        "\r\n",
        "O corpus UOL AES-PT é um corpus de redações no estilo do ENEM. Cada redação possui um tópico e um conjunto de redações relacionadas. Nesse corpus, existem vários tópicos e suas respectivas redações. O objetivo é predizer a nota final de cada redação de acordo com o grupo de notas 0, 200, 400, 600, 800 e 1000. Para mais informações e download dos dados, acesse o link: https://github.com/evelinamorim/aes-pt.\r\n",
        "\r\n",
        "O corpus TweetSentBr é um corpus em português de tweets. Cada tweet está rotulado com uma das classes: positivo, negativo e neutro. Para mais informações e download do corpus, acesse o link https://bitbucket.org/HBrum/tweetsentbr/src/master/."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRmPygTVlXTG"
      },
      "source": [
        "A base escolhido é a do corpus UTL é um corpus com críticas de filmes e apps coletadas automaticamente de sites. As classes são: positiva ou negativa. Assim o usuário pode ter gostado ou não gostado do produto. Referência: https://github.com/RogerFig/UTLCorpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AD5iPWAJX1aX"
      },
      "source": [
        "## Importanto as bibliotecas necessárias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMEXm13uX54E",
        "outputId": "9b388678-1dda-4c9b-d4ad-5e1c7118893b"
      },
      "source": [
        "### # montando o drive do google onde o corpus se encontra\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n",
        "\r\n",
        "### Entrando no diretório\r\n",
        "%cd drive/MyDrive/Colab Notebooks/NLP/trabalho"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/Colab Notebooks/NLP/trabalho\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xizqZPgGX1aX",
        "outputId": "245e5089-3bff-4d69-f56b-2b32cb93ce51"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "%matplotlib inline\n",
        "#nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "#nltk.download('wordnet')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9Tkls2bX1aa"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import time # usado para calcular o tempo de processamento de algumas partes do código\n",
        "import string # usado para concatenar string no stop words\n",
        "\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# REPRESENTAÇÕES\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import TfidfModel\n",
        "from gensim.corpora import Dictionary # para uso do TF-IDF\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8cDe6KMX1aa"
      },
      "source": [
        "## Abrindo o arquivo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "aDWnK_yiZgu8",
        "outputId": "eaec08e4-f7c1-4fda-ff74-9752eff89a71"
      },
      "source": [
        "# abrindo o arquivo no colab\r\n",
        "df_util = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/NLP/trabalho/dados/UTLCorpus.csv')\r\n",
        "df_util.head(3)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>domain</th>\n",
              "      <th>object</th>\n",
              "      <th>author</th>\n",
              "      <th>text</th>\n",
              "      <th>likes</th>\n",
              "      <th>unlikes</th>\n",
              "      <th>stars</th>\n",
              "      <th>date</th>\n",
              "      <th>collect_date</th>\n",
              "      <th>replies</th>\n",
              "      <th>favorite</th>\n",
              "      <th>want_see</th>\n",
              "      <th>recommend</th>\n",
              "      <th>see</th>\n",
              "      <th>internal_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>movies</td>\n",
              "      <td>007-cassino-royale-t23</td>\n",
              "      <td>0</td>\n",
              "      <td>Um dos melhores do 007</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>8 anos atrás</td>\n",
              "      <td>20-04-2019 06:20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>movies</td>\n",
              "      <td>007-cassino-royale-t23</td>\n",
              "      <td>1</td>\n",
              "      <td>assisti só pela metade..do meio ao fim, o film...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>8 anos atrás</td>\n",
              "      <td>20-04-2019 06:20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>movies</td>\n",
              "      <td>007-cassino-royale-t23</td>\n",
              "      <td>2</td>\n",
              "      <td>foi um dos filmes mais violentos q já vi. mas ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>8 anos atrás</td>\n",
              "      <td>20-04-2019 06:20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  domain                  object  ...  recommend  see  internal_id\n",
              "0           0  movies  007-cassino-royale-t23  ...        NaN  1.0            0\n",
              "1           1  movies  007-cassino-royale-t23  ...        NaN  1.0            1\n",
              "2           2  movies  007-cassino-royale-t23  ...        NaN  1.0            2\n",
              "\n",
              "[3 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zg6ztZNiX1ab"
      },
      "source": [
        "# abrindo o arquivo localmente\n",
        "# df_util = pd.read_csv(r'C:\\Users\\Herica\\Documents\\Herica\\Pos\\GitHub\\base\\NLP\\UTLCorpus.csv')\n",
        "# df_util.head(3)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDjKfg7ZX1ab"
      },
      "source": [
        "# Analisando os dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzQYH6hFX1ac",
        "outputId": "8b005a0c-041e-49e5-8a3b-59a0c339e92e"
      },
      "source": [
        "# Verificando quais os tipos da feature domain\n",
        "df_util.domain.unique()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['movies', 'apps'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBey34QFX1ac"
      },
      "source": [
        "qtde_moveis = len(df_util[df_util['domain'] == 'movies'])\n",
        "qtde_apps = len(df_util[df_util['domain'] == 'apps'])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJT_WLv_X1ad"
      },
      "source": [
        "# Selecionando somente o registros do tipo movies\n",
        "df_movies = df_util[df_util['domain'] == 'movies']"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cW0pRDlyX1ad"
      },
      "source": [
        "## Selecionando as features de interesse"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "ApPfEhv_X1ae",
        "outputId": "2c2587aa-4069-4dc1-87e6-be5a43d72c59"
      },
      "source": [
        "df_movies = df_movies[['object', 'text', 'likes']]\n",
        "df_movies.head(3)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>object</th>\n",
              "      <th>text</th>\n",
              "      <th>likes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>007-cassino-royale-t23</td>\n",
              "      <td>Um dos melhores do 007</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>007-cassino-royale-t23</td>\n",
              "      <td>assisti só pela metade..do meio ao fim, o film...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>007-cassino-royale-t23</td>\n",
              "      <td>foi um dos filmes mais violentos q já vi. mas ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   object  ... likes\n",
              "0  007-cassino-royale-t23  ...     0\n",
              "1  007-cassino-royale-t23  ...     0\n",
              "2  007-cassino-royale-t23  ...     0\n",
              "\n",
              "[3 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5wE53KyX1ae"
      },
      "source": [
        "DESCRIÇÃO DAS FEATURES QUE SERÃO UTILIZADAS NO TRABALHO\n",
        "\n",
        "object -> Nome do filme\n",
        "\n",
        "text -> avalianção dos filme feita pelos usuários"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HevXuRfvX1ae",
        "outputId": "4e654095-e2a9-455c-80aa-ef689f6761e6"
      },
      "source": [
        "df_movies.columns"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['object', 'text', 'likes'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KQDwIAgX1af",
        "outputId": "1949135e-87cb-4142-9412-84ae06b380ad"
      },
      "source": [
        "# analisando a qtde de filmes avaliados\n",
        "df_movies['object'].value_counts()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "cisne-negro-t16349                               7839\n",
              "os-vingadores-t15324                             7059\n",
              "as-vantagens-de-ser-invisivel-t29850             6940\n",
              "batman-o-cavaleiro-das-trevas-ressurge-t19914    6413\n",
              "a-origem-t10852                                  5801\n",
              "                                                 ... \n",
              "mega-man-t191384                                    1\n",
              "leste-oeste-t112105                                 1\n",
              "the-legend-of-thunder-t193710                       1\n",
              "the-world-of-us-t211052                             1\n",
              "um-homem-e-sua-paixao-t108598                       1\n",
              "Name: object, Length: 4203, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "6LF8hJPZ3H5O",
        "outputId": "6ec51a87-2f98-4872-adc5-b0ebf53323e5"
      },
      "source": [
        "# convertendo as palavras do campo texto para lowcase\r\n",
        "df_movies['text_lower'] = df_movies['text'].astype('str').str.lower()\r\n",
        "df_movies[['text','text_lower']].head(3)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>text_lower</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Um dos melhores do 007</td>\n",
              "      <td>um dos melhores do 007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>assisti só pela metade..do meio ao fim, o film...</td>\n",
              "      <td>assisti só pela metade..do meio ao fim, o film...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>foi um dos filmes mais violentos q já vi. mas ...</td>\n",
              "      <td>foi um dos filmes mais violentos q já vi. mas ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text                                         text_lower\n",
              "0                             Um dos melhores do 007                             um dos melhores do 007\n",
              "1  assisti só pela metade..do meio ao fim, o film...  assisti só pela metade..do meio ao fim, o film...\n",
              "2  foi um dos filmes mais violentos q já vi. mas ...  foi um dos filmes mais violentos q já vi. mas ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHUiE75CpZ7b"
      },
      "source": [
        "### Definindo a classe que irá classificar a avaliação do filme\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "bbD-SwjuoGfR",
        "outputId": "7bf3661e-3b4f-4bef-b6d9-484ce7f1a88f"
      },
      "source": [
        "def define_classe(classificacao:float)->int:\r\n",
        "  tokens = nltk.word_tokenize(row['like'])\r\n",
        "    if (like = 0.):\r\n",
        "        return 0 # classe negativa\r\n",
        "    else:\r\n",
        "        return 1 # classe positiva\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def identify_tokens(row):\r\n",
        "    tokens = nltk.word_tokenize(row['text_lower'])    \r\n",
        "    #print (tokens)\r\n",
        "    # pegar somente palavra, sem pontuação\r\n",
        "    token_words = [w for w in tokens if w.isalnum()] # isalnum (não remove caracteres especiais e números) - isalpha (removeremos palavras / caracteres não alfanuméricos (como números e pontuação)\r\n",
        "    return token_words"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-ac65aa5fc34e>\"\u001b[0;36m, line \u001b[0;32m8\u001b[0m\n\u001b[0;31m    if (like = 0.):\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-02c873a664bf>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    if (like = 0.):\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtmnWxEapiiC"
      },
      "source": [
        "start = time.time()\r\n",
        "df_movies['classificacao'] = df_movies.apply(define_classe, axis=1)\r\n",
        "print (\"tempo gasto: \", (time.time() - start))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QLDjXjQX1af"
      },
      "source": [
        "## Verificando e deletando registros nulos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwV-0Ry3X1ag",
        "outputId": "cb7feb19-3ed1-43dc-e6f7-b19007d7a17f"
      },
      "source": [
        "df_movies.isnull().sum()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "object          0\n",
              "text          956\n",
              "likes           0\n",
              "text_lower      0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rVXhpQWX1ag",
        "outputId": "60667df8-daf9-4c19-fff5-958181f18ab0"
      },
      "source": [
        "# deletando os registros nulos\n",
        "qtde_anterior_nulos = df_movies.shape[0]\n",
        "df_movies.dropna(subset=['text'], inplace = True)\n",
        "qtde_pos_exclusao_nulos = df_movies.shape[0]\n",
        "\n",
        "print (\"Antes:\", qtde_anterior_nulos, 'Depois:', qtde_pos_exclusao_nulos, 'Diferença:', qtde_anterior_nulos-qtde_pos_exclusao_nulos)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Antes: 1839851 Depois: 1838895 Diferença: 956\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxQoPiQBX1ah",
        "outputId": "0e74be4b-14a0-43ee-ccf4-9c6495290de5"
      },
      "source": [
        "# Validando a exclusão\n",
        "df_movies.isnull().sum()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "object        0\n",
              "text          0\n",
              "likes         0\n",
              "text_lower    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZMwZtzEX1ah"
      },
      "source": [
        "## Pré-processando os dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBSL7EonX1al",
        "outputId": "68c92cd7-0bca-40df-8129-51841c0c4965"
      },
      "source": [
        "# definindo e visualizando as stop-words\n",
        "stp_words = nltk.corpus.stopwords.words('portuguese')\n",
        "stop_words = stp_words +' '.join(string.punctuation).split(' ')+[\"``\",\"''\",\"“\",'”', \"'\", \"]\" , \"[\" , \",\" , \".\"]\n",
        "\n",
        "table = str.maketrans(dict.fromkeys(string.punctuation)) \n",
        "\n",
        "\n",
        "print('Visualizando as stopwords \\n', stop_words)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Visualizando as stopwords \n",
            " ['de', 'a', 'o', 'que', 'e', 'é', 'do', 'da', 'em', 'um', 'para', 'com', 'não', 'uma', 'os', 'no', 'se', 'na', 'por', 'mais', 'as', 'dos', 'como', 'mas', 'ao', 'ele', 'das', 'à', 'seu', 'sua', 'ou', 'quando', 'muito', 'nos', 'já', 'eu', 'também', 'só', 'pelo', 'pela', 'até', 'isso', 'ela', 'entre', 'depois', 'sem', 'mesmo', 'aos', 'seus', 'quem', 'nas', 'me', 'esse', 'eles', 'você', 'essa', 'num', 'nem', 'suas', 'meu', 'às', 'minha', 'numa', 'pelos', 'elas', 'qual', 'nós', 'lhe', 'deles', 'essas', 'esses', 'pelas', 'este', 'dele', 'tu', 'te', 'vocês', 'vos', 'lhes', 'meus', 'minhas', 'teu', 'tua', 'teus', 'tuas', 'nosso', 'nossa', 'nossos', 'nossas', 'dela', 'delas', 'esta', 'estes', 'estas', 'aquele', 'aquela', 'aqueles', 'aquelas', 'isto', 'aquilo', 'estou', 'está', 'estamos', 'estão', 'estive', 'esteve', 'estivemos', 'estiveram', 'estava', 'estávamos', 'estavam', 'estivera', 'estivéramos', 'esteja', 'estejamos', 'estejam', 'estivesse', 'estivéssemos', 'estivessem', 'estiver', 'estivermos', 'estiverem', 'hei', 'há', 'havemos', 'hão', 'houve', 'houvemos', 'houveram', 'houvera', 'houvéramos', 'haja', 'hajamos', 'hajam', 'houvesse', 'houvéssemos', 'houvessem', 'houver', 'houvermos', 'houverem', 'houverei', 'houverá', 'houveremos', 'houverão', 'houveria', 'houveríamos', 'houveriam', 'sou', 'somos', 'são', 'era', 'éramos', 'eram', 'fui', 'foi', 'fomos', 'foram', 'fora', 'fôramos', 'seja', 'sejamos', 'sejam', 'fosse', 'fôssemos', 'fossem', 'for', 'formos', 'forem', 'serei', 'será', 'seremos', 'serão', 'seria', 'seríamos', 'seriam', 'tenho', 'tem', 'temos', 'tém', 'tinha', 'tínhamos', 'tinham', 'tive', 'teve', 'tivemos', 'tiveram', 'tivera', 'tivéramos', 'tenha', 'tenhamos', 'tenham', 'tivesse', 'tivéssemos', 'tivessem', 'tiver', 'tivermos', 'tiverem', 'terei', 'terá', 'teremos', 'terão', 'teria', 'teríamos', 'teriam', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '``', \"''\", '“', '”', \"'\", ']', '[', ',', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6YcliccX1ai"
      },
      "source": [
        "# Definindo as funções que serão utilizadas\n",
        "\n",
        "# Tokenizando\n",
        "def identify_tokens(row):\n",
        "    tokens = nltk.word_tokenize(row['text_lower'])    \n",
        "    #print (tokens)\n",
        "    # pegar somente palavra, sem pontuação\n",
        "    token_words = [w for w in tokens if w.isalnum()] # isalnum (não remove caracteres especiais e números) - isalpha (removeremos palavras / caracteres não alfanuméricos (como números e pontuação)\n",
        "    return token_words\n",
        "\n",
        "# Removendo as stop-words\n",
        "def remove_stops(row):\n",
        "    my_list = row['text_tokenizado']\n",
        "    meaningful_words = [w for w in my_list if not w in stop_words]\n",
        "    return (meaningful_words)\n",
        "\n",
        "# Cria uma lista de palavras\n",
        "def lista_palavras(row):\n",
        "  lista_de_palavras = corpora.Dictionary(row['text_stopwords']) \n",
        "  print (lista_de_palavras)\n",
        "  return lista_de_palavras\n",
        "\n",
        "# cria um dicionário de palavras\n",
        "def dicionario(row):\n",
        "  #print ('row', row)\n",
        "  #print ('linha', row['text_stopwords'])\n",
        "  #print ('tipo do row', type(row))\n",
        "  #print ('tipo do row.text_stopwords', type(row['text_stopwords']))\n",
        "  #dicionario = corpora.Dictionary(row['text_stopwords']) \n",
        "  \n",
        "  dicionario_bow = dicionario.doc2bow(str(row['text_stopwords']), allow_update=True)\n",
        "  return dicionario_bow\n",
        "\n",
        "# Contar qtde de palavras\n",
        "def contar_palavras(row):\n",
        "  palavraCont = {} # criando um dicionário\n",
        "  palavras_lista = word_tokenize(str(row['text_stopwords'])) # criando uma lista de palavras para cada registro\n",
        "  #print (type(palavraCont), type(palavras_lista))\n",
        "  #print (palavras_lista)\n",
        "  for palavra in palavras_lista:\n",
        "    if palavra in palavraCont:\n",
        "        palavraCont[palavra] += 1\n",
        "    else:\n",
        "        palavraCont[palavra] = 1\n",
        "    #print ('palavraCont:', palavraCont)\n",
        "  return palavraCont"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTkcaA_rLNXD"
      },
      "source": [
        "#type(str(df_movies['text_stopwords']))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNwxytGkEgQ7"
      },
      "source": [
        "\r\n",
        "#dicionario = corpora.Dictionary()\r\n",
        "#df_movies['bow'] = df_movies.apply(dicionario, axis=1)\r\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cz3J7KRCX1ai"
      },
      "source": [
        "# resetando os índices\n",
        "df_movies = df_movies.reset_index()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vf8N3SPsX1aj"
      },
      "source": [
        "### Tokenizando"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBr8A7FCX1aj",
        "outputId": "de2dc2e6-52f9-48d7-9c2a-ace6070a79d8"
      },
      "source": [
        "start = time.time()\n",
        "df_movies['text_tokenizado'] = df_movies.apply(identify_tokens, axis=1)\n",
        "print (\"tempo gasto: \", (time.time() - start))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tempo gasto:  672.744140625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUuwFkK5goG6",
        "outputId": "b2d8e3f0-f01a-43b4-db6d-2974bdf6d740"
      },
      "source": [
        "type(df_movies['text_tokenizado'].values)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJOjCAUveWSM",
        "outputId": "f98e0e1e-4dec-4d7d-e59e-2220041df17f"
      },
      "source": [
        "type(list(str(df_movies['text_tokenizado'])))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "GKP6xaknX1aj",
        "outputId": "789bf58e-a99f-4de1-a274-9d1b9543f909"
      },
      "source": [
        "# validando a tokenização\n",
        "df_movies[['text', 'text_lower','text_tokenizado']][1:3].head(3)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>text_lower</th>\n",
              "      <th>text_tokenizado</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>assisti só pela metade..do meio ao fim, o film...</td>\n",
              "      <td>assisti só pela metade..do meio ao fim, o film...</td>\n",
              "      <td>[assisti, só, pela, meio, ao, fim, o, filme, p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>foi um dos filmes mais violentos q já vi. mas ...</td>\n",
              "      <td>foi um dos filmes mais violentos q já vi. mas ...</td>\n",
              "      <td>[foi, um, dos, filmes, mais, violentos, q, já,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  ...                                    text_tokenizado\n",
              "1  assisti só pela metade..do meio ao fim, o film...  ...  [assisti, só, pela, meio, ao, fim, o, filme, p...\n",
              "2  foi um dos filmes mais violentos q já vi. mas ...  ...  [foi, um, dos, filmes, mais, violentos, q, já,...\n",
              "\n",
              "[2 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaTD20WXX1ak"
      },
      "source": [
        "### Convertento os tokes para minúsculo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "estJOY5nX1ak"
      },
      "source": [
        "#start = time.time()\n",
        "#df_movies['text_lower'] = df_movies['text_tokenizado'].astype('str').str.lower()\n",
        "#df_movies.head(3)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cteS1B3XX1al"
      },
      "source": [
        "### Removendo as stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrPo2ZDjX1al",
        "outputId": "1a79cc7c-6ad6-4031-8f6d-483aaf45e883"
      },
      "source": [
        "start = time.time()\n",
        "df_movies['text_stopwords'] = df_movies.apply(remove_stops, axis=1) # chamando a função que remove as stopwords\n",
        "print (\"tempo gasto: \", (time.time() - start))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tempo gasto:  120.67205572128296\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "lYs93yvGX1am",
        "outputId": "c003fc7f-13f9-4720-8379-c93ddaf5b4f4"
      },
      "source": [
        "# Validando\n",
        "df_movies[['text_tokenizado','text_stopwords']].head(3)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_tokenizado</th>\n",
              "      <th>text_stopwords</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[um, dos, melhores, do, 007]</td>\n",
              "      <td>[melhores, 007]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[assisti, só, pela, meio, ao, fim, o, filme, p...</td>\n",
              "      <td>[assisti, meio, fim, filme, parece, ser, mt, b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[foi, um, dos, filmes, mais, violentos, q, já,...</td>\n",
              "      <td>[filmes, violentos, q, vi, booom, demaais]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     text_tokenizado                                     text_stopwords\n",
              "0                       [um, dos, melhores, do, 007]                                    [melhores, 007]\n",
              "1  [assisti, só, pela, meio, ao, fim, o, filme, p...  [assisti, meio, fim, filme, parece, ser, mt, b...\n",
              "2  [foi, um, dos, filmes, mais, violentos, q, já,...         [filmes, violentos, q, vi, booom, demaais]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0EHM3pYgWQ4",
        "outputId": "3d34071b-543f-4f28-8582-79c2f70a8b79"
      },
      "source": [
        "df_movies['text_tokenizado'][3]"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['adoroo', 'esse', 'filme']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bn4820kZgeB8",
        "outputId": "cc8a00a8-121e-4d2b-bfc1-95c8ff5d4e6d"
      },
      "source": [
        "df_movies['text_stopwords'][3]"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['adoroo', 'filme']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_6wqyUeX1am"
      },
      "source": [
        "### Salvando o resultado num dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXz_fbVYX1am"
      },
      "source": [
        "df2 = df_movies[['object', 'text_stopwords']]\n",
        "\n",
        "df2.to_csv(\"/content/drive/MyDrive/Colab Notebooks/NLP/trabalho/dados/df_util_tokenizado.csv\",index=False)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jF25w7UGX0KQ"
      },
      "source": [
        "### Abrindo o arquivo TOKENIZADO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbdxUi1lX1an",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "c1879b1a-fa66-4939-a7b7-c8162c5848c2"
      },
      "source": [
        "df_processado = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/NLP/trabalho/dados/df_util_tokenizado.csv\")\n",
        "df_processado.head(3)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>object</th>\n",
              "      <th>text_stopwords</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>007-cassino-royale-t23</td>\n",
              "      <td>['melhores', '007']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>007-cassino-royale-t23</td>\n",
              "      <td>['assisti', 'meio', 'fim', 'filme', 'parece', ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>007-cassino-royale-t23</td>\n",
              "      <td>['filmes', 'violentos', 'q', 'vi', 'booom', 'd...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   object                                     text_stopwords\n",
              "0  007-cassino-royale-t23                                ['melhores', '007']\n",
              "1  007-cassino-royale-t23  ['assisti', 'meio', 'fim', 'filme', 'parece', ...\n",
              "2  007-cassino-royale-t23  ['filmes', 'violentos', 'q', 'vi', 'booom', 'd..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DP3Nap7TPe13"
      },
      "source": [
        "## inicio testes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6aAdL2f7P3f"
      },
      "source": [
        "## Contando a qtde de palavras de cada registro"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "47pzYUBC4uWU",
        "outputId": "063927f5-5ce8-412a-b00e-617227622916"
      },
      "source": [
        "df_processado2"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>object</th>\n",
              "      <th>text_stopwords</th>\n",
              "      <th>qtde_palavras</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>007-cassino-royale-t23</td>\n",
              "      <td>['melhores', '007']</td>\n",
              "      <td>{'[': 1, \"'melhores\": 1, \"'\": 2, ',': 1, \"'007...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>007-cassino-royale-t23</td>\n",
              "      <td>['assisti', 'meio', 'fim', 'filme', 'parece', ...</td>\n",
              "      <td>{'[': 1, \"'assisti\": 1, \"'\": 15, ',': 14, \"'me...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>007-cassino-royale-t23</td>\n",
              "      <td>['filmes', 'violentos', 'q', 'vi', 'booom', 'd...</td>\n",
              "      <td>{'[': 1, \"'filmes\": 1, \"'\": 6, ',': 5, \"'viole...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>007-cassino-royale-t23</td>\n",
              "      <td>['adoroo', 'filme']</td>\n",
              "      <td>{'[': 1, \"'adoroo\": 1, \"'\": 2, ',': 1, \"'filme...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>007-cassino-royale-t23</td>\n",
              "      <td>['daniel', 'crieg', 'carisma', 'zero', '007']</td>\n",
              "      <td>{'[': 1, \"'daniel\": 1, \"'\": 5, ',': 4, \"'crieg...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1838890</th>\n",
              "      <td>zumbis-do-kung-fu-t38547</td>\n",
              "      <td>['certeza', 'dois', 'dubladores', 'mínima', 'n...</td>\n",
              "      <td>{'[': 1, \"'certeza\": 1, \"'\": 12, ',': 11, \"'do...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1838891</th>\n",
              "      <td>zumbis-do-kung-fu-t38547</td>\n",
              "      <td>['divertidão', 'tão', 'mal', 'feito']</td>\n",
              "      <td>{'[': 1, \"'divertidão\": 1, \"'\": 4, ',': 3, \"'t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1838892</th>\n",
              "      <td>zumbis-do-kung-fu-t38547</td>\n",
              "      <td>['divertido', 'direito', 'musicas', 'james', '...</td>\n",
              "      <td>{'[': 1, \"'divertido\": 1, \"'\": 11, ',': 10, \"'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1838893</th>\n",
              "      <td>zumbis-do-kung-fu-t38547</td>\n",
              "      <td>['filme', 'engraçado', 'vi', 'vida']</td>\n",
              "      <td>{'[': 1, \"'filme\": 1, \"'\": 4, ',': 3, \"'engraç...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1838894</th>\n",
              "      <td>zumbis-do-kung-fu-t38547</td>\n",
              "      <td>['extremamente', 'tosco', 'tosco', 'divertido'...</td>\n",
              "      <td>{'[': 1, \"'extremamente\": 1, \"'\": 5, ',': 4, \"...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1838895 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                           object  ...                                      qtde_palavras\n",
              "0          007-cassino-royale-t23  ...  {'[': 1, \"'melhores\": 1, \"'\": 2, ',': 1, \"'007...\n",
              "1          007-cassino-royale-t23  ...  {'[': 1, \"'assisti\": 1, \"'\": 15, ',': 14, \"'me...\n",
              "2          007-cassino-royale-t23  ...  {'[': 1, \"'filmes\": 1, \"'\": 6, ',': 5, \"'viole...\n",
              "3          007-cassino-royale-t23  ...  {'[': 1, \"'adoroo\": 1, \"'\": 2, ',': 1, \"'filme...\n",
              "4          007-cassino-royale-t23  ...  {'[': 1, \"'daniel\": 1, \"'\": 5, ',': 4, \"'crieg...\n",
              "...                           ...  ...                                                ...\n",
              "1838890  zumbis-do-kung-fu-t38547  ...  {'[': 1, \"'certeza\": 1, \"'\": 12, ',': 11, \"'do...\n",
              "1838891  zumbis-do-kung-fu-t38547  ...  {'[': 1, \"'divertidão\": 1, \"'\": 4, ',': 3, \"'t...\n",
              "1838892  zumbis-do-kung-fu-t38547  ...  {'[': 1, \"'divertido\": 1, \"'\": 11, ',': 10, \"'...\n",
              "1838893  zumbis-do-kung-fu-t38547  ...  {'[': 1, \"'filme\": 1, \"'\": 4, ',': 3, \"'engraç...\n",
              "1838894  zumbis-do-kung-fu-t38547  ...  {'[': 1, \"'extremamente\": 1, \"'\": 5, ',': 4, \"...\n",
              "\n",
              "[1838895 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "khY0m3QD37XM",
        "outputId": "339b6856-5546-4134-e12f-7b3dcf048c6c"
      },
      "source": [
        "df_processado2.text_stopwords[df_processado2.indice == 4]"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-770e6e25ee94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_processado2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_stopwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_processado2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindice\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5139\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5140\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5141\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'indice'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "id": "X4OhqOWh3jKx",
        "outputId": "f89975b6-b82c-4be7-8cd7-e198305442fd"
      },
      "source": [
        "print(df_processado2[3].apply(contar_palavras, axis=1))\r\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2898\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 3",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-4d088e2b3fc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_processado\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontar_palavras\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2904\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2905\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2906\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2907\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2908\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2898\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2900\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2902\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 3"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZ-Lqt_kcamE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd79cb86-8db9-4868-d6e3-0ce748750af5"
      },
      "source": [
        "start = time.time()\r\n",
        "df_processado['qtde_palavras'] = df_processado.apply(contar_palavras, axis=1)\r\n",
        "print (\"tempo gasto: \", (time.time() - start))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tempo gasto:  596.9280700683594\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BD6Vob8LrRV8",
        "outputId": "afcce6d0-0916-4431-a5ad-bde393f5b276"
      },
      "source": [
        "df_processado['qtde_palavras']"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0          {'[': 1, ''melhores': 1, ''': 2, ',': 1, ''007...\n",
              "1          {'[': 1, ''assisti': 1, ''': 15, ',': 14, ''me...\n",
              "2          {'[': 1, ''filmes': 1, ''': 6, ',': 5, ''viole...\n",
              "3          {'[': 1, ''adoroo': 1, ''': 2, ',': 1, ''filme...\n",
              "4          {'[': 1, ''daniel': 1, ''': 5, ',': 4, ''crieg...\n",
              "                                 ...                        \n",
              "1838890    {'[': 1, ''certeza': 1, ''': 12, ',': 11, ''do...\n",
              "1838891    {'[': 1, ''divertidão': 1, ''': 4, ',': 3, ''t...\n",
              "1838892    {'[': 1, ''divertido': 1, ''': 11, ',': 10, ''...\n",
              "1838893    {'[': 1, ''filme': 1, ''': 4, ',': 3, ''engraç...\n",
              "1838894    {'[': 1, ''extremamente': 1, ''': 5, ',': 4, '...\n",
              "Name: qtde_palavras, Length: 1838895, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOiKwJ7W1ePm",
        "outputId": "51be2129-0579-454e-ba86-b56ddcc8aa1d"
      },
      "source": [
        "df_processado['qtde_palavras'][3]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{\"'\": 2, \"'adoroo\": 1, \"'filme\": 1, ',': 1, '[': 1, ']': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKPQCU3BI5Fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "d6ce0c6c-7bed-4c72-ffc7-cc490b0f2a05"
      },
      "source": [
        "df_processado"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>object</th>\n",
              "      <th>text_stopwords</th>\n",
              "      <th>qtde_palavras</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>007-cassino-royale-t23</td>\n",
              "      <td>['melhores', '007']</td>\n",
              "      <td>{'[': 1, ''melhores': 1, ''': 2, ',': 1, ''007...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>007-cassino-royale-t23</td>\n",
              "      <td>['assisti', 'meio', 'fim', 'filme', 'parece', ...</td>\n",
              "      <td>{'[': 1, ''assisti': 1, ''': 15, ',': 14, ''me...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>007-cassino-royale-t23</td>\n",
              "      <td>['filmes', 'violentos', 'q', 'vi', 'booom', 'd...</td>\n",
              "      <td>{'[': 1, ''filmes': 1, ''': 6, ',': 5, ''viole...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>007-cassino-royale-t23</td>\n",
              "      <td>['adoroo', 'filme']</td>\n",
              "      <td>{'[': 1, ''adoroo': 1, ''': 2, ',': 1, ''filme...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>007-cassino-royale-t23</td>\n",
              "      <td>['daniel', 'crieg', 'carisma', 'zero', '007']</td>\n",
              "      <td>{'[': 1, ''daniel': 1, ''': 5, ',': 4, ''crieg...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1838890</th>\n",
              "      <td>zumbis-do-kung-fu-t38547</td>\n",
              "      <td>['certeza', 'dois', 'dubladores', 'mínima', 'n...</td>\n",
              "      <td>{'[': 1, ''certeza': 1, ''': 12, ',': 11, ''do...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1838891</th>\n",
              "      <td>zumbis-do-kung-fu-t38547</td>\n",
              "      <td>['divertidão', 'tão', 'mal', 'feito']</td>\n",
              "      <td>{'[': 1, ''divertidão': 1, ''': 4, ',': 3, ''t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1838892</th>\n",
              "      <td>zumbis-do-kung-fu-t38547</td>\n",
              "      <td>['divertido', 'direito', 'musicas', 'james', '...</td>\n",
              "      <td>{'[': 1, ''divertido': 1, ''': 11, ',': 10, ''...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1838893</th>\n",
              "      <td>zumbis-do-kung-fu-t38547</td>\n",
              "      <td>['filme', 'engraçado', 'vi', 'vida']</td>\n",
              "      <td>{'[': 1, ''filme': 1, ''': 4, ',': 3, ''engraç...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1838894</th>\n",
              "      <td>zumbis-do-kung-fu-t38547</td>\n",
              "      <td>['extremamente', 'tosco', 'tosco', 'divertido'...</td>\n",
              "      <td>{'[': 1, ''extremamente': 1, ''': 5, ',': 4, '...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1838895 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                           object  ...                                      qtde_palavras\n",
              "0          007-cassino-royale-t23  ...  {'[': 1, ''melhores': 1, ''': 2, ',': 1, ''007...\n",
              "1          007-cassino-royale-t23  ...  {'[': 1, ''assisti': 1, ''': 15, ',': 14, ''me...\n",
              "2          007-cassino-royale-t23  ...  {'[': 1, ''filmes': 1, ''': 6, ',': 5, ''viole...\n",
              "3          007-cassino-royale-t23  ...  {'[': 1, ''adoroo': 1, ''': 2, ',': 1, ''filme...\n",
              "4          007-cassino-royale-t23  ...  {'[': 1, ''daniel': 1, ''': 5, ',': 4, ''crieg...\n",
              "...                           ...  ...                                                ...\n",
              "1838890  zumbis-do-kung-fu-t38547  ...  {'[': 1, ''certeza': 1, ''': 12, ',': 11, ''do...\n",
              "1838891  zumbis-do-kung-fu-t38547  ...  {'[': 1, ''divertidão': 1, ''': 4, ',': 3, ''t...\n",
              "1838892  zumbis-do-kung-fu-t38547  ...  {'[': 1, ''divertido': 1, ''': 11, ',': 10, ''...\n",
              "1838893  zumbis-do-kung-fu-t38547  ...  {'[': 1, ''filme': 1, ''': 4, ',': 3, ''engraç...\n",
              "1838894  zumbis-do-kung-fu-t38547  ...  {'[': 1, ''extremamente': 1, ''': 5, ',': 4, '...\n",
              "\n",
              "[1838895 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwpYxDcRR81V"
      },
      "source": [
        "# SALVANDO\r\n",
        "df3 = df_processado[['object', 'text_stopwords', 'qtde_palavras']]\r\n",
        "\r\n",
        "df3.to_csv(\"/content/drive/MyDrive/Colab Notebooks/NLP/trabalho/dados/df_util_tokenizado2.csv\",index=False)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "wicjXKlN3B_D",
        "outputId": "26b1dab6-90a8-472d-d4f5-9b852938ea67"
      },
      "source": [
        "df_processado2 = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/NLP/trabalho/dados/df_util_tokenizado2.csv\")\r\n",
        "df_processado2.head(3)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>object</th>\n",
              "      <th>text_stopwords</th>\n",
              "      <th>qtde_palavras</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>007-cassino-royale-t23</td>\n",
              "      <td>['melhores', '007']</td>\n",
              "      <td>{'[': 1, \"'melhores\": 1, \"'\": 2, ',': 1, \"'007...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>007-cassino-royale-t23</td>\n",
              "      <td>['assisti', 'meio', 'fim', 'filme', 'parece', ...</td>\n",
              "      <td>{'[': 1, \"'assisti\": 1, \"'\": 15, ',': 14, \"'me...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>007-cassino-royale-t23</td>\n",
              "      <td>['filmes', 'violentos', 'q', 'vi', 'booom', 'd...</td>\n",
              "      <td>{'[': 1, \"'filmes\": 1, \"'\": 6, ',': 5, \"'viole...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   object  ...                                      qtde_palavras\n",
              "0  007-cassino-royale-t23  ...  {'[': 1, \"'melhores\": 1, \"'\": 2, ',': 1, \"'007...\n",
              "1  007-cassino-royale-t23  ...  {'[': 1, \"'assisti\": 1, \"'\": 15, ',': 14, \"'me...\n",
              "2  007-cassino-royale-t23  ...  {'[': 1, \"'filmes\": 1, \"'\": 6, ',': 5, \"'viole...\n",
              "\n",
              "[3 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VJFU8CMEJtP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d346066-8e48-4f80-9bd1-3829323605bd"
      },
      "source": [
        "# cria uma lista de palavras para cada registro\r\n",
        "start = time.time()\r\n",
        "df_processado2['lista_de_palavras'] = df_processado.apply(contar_palavras, axis=1) # chamando a função que cria a lista de palavras para cada registro\r\n",
        "print (\"tempo gasto: \", (time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tempo gasto:  603.3460853099823\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGCb_cZ6MBBs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "f78c4163-6aa0-4bca-a03e-1390d400a093"
      },
      "source": [
        "df_processado[['text_stopwords','lista_de_palavras']].head(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_stopwords</th>\n",
              "      <th>lista_de_palavras</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[melhores, 007]</td>\n",
              "      <td>{'[': 1, ''melhores': 1, ''': 2, ',': 1, ''007...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[assisti, meio, fim, filme, parece, ser, mt, b...</td>\n",
              "      <td>{'[': 1, ''assisti': 1, ''': 15, ',': 14, ''me...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[filmes, violentos, q, vi, booom, demaais]</td>\n",
              "      <td>{'[': 1, ''filmes': 1, ''': 6, ',': 5, ''viole...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                      text_stopwords                                  lista_de_palavras\n",
              "0                                    [melhores, 007]  {'[': 1, ''melhores': 1, ''': 2, ',': 1, ''007...\n",
              "1  [assisti, meio, fim, filme, parece, ser, mt, b...  {'[': 1, ''assisti': 1, ''': 15, ',': 14, ''me...\n",
              "2         [filmes, violentos, q, vi, booom, demaais]  {'[': 1, ''filmes': 1, ''': 6, ',': 5, ''viole..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xi-uuJU1Pkit",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "c62008ae-7666-4e7b-d745-98394ecacb30"
      },
      "source": [
        "from gensim import corpora\r\n",
        "\r\n",
        "# cria um dicionário de palavras\r\n",
        "df_processado['dicionario'] = df_processado.apply(dicionario, axis=1) # chamando a função que cria um dicionário para cada registro"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "row index                                                                0\n",
            "object                                          007-cassino-royale-t23\n",
            "text                                            Um dos melhores do 007\n",
            "likes                                                                0\n",
            "text_lower                                      um dos melhores do 007\n",
            "text_tokenizado                           [um, dos, melhores, do, 007]\n",
            "text_stopwords                                         [melhores, 007]\n",
            "lista_de_palavras    {'[': 1, ''melhores': 1, ''': 2, ',': 1, ''007...\n",
            "Name: 0, dtype: object\n",
            "linha ['melhores', '007']\n",
            "tipo do row <class 'pandas.core.series.Series'>\n",
            "tipo do row.text_stopwords <class 'list'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-fb37a8ad2948>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# cria um dicionário de palavras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf_movies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dicionario'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_movies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdicionario\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# chamando a função que cria um dicionário para cada registro\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwds)\u001b[0m\n\u001b[1;32m   7550\u001b[0m             \u001b[0mkwds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7551\u001b[0m         )\n\u001b[0;32m-> 7552\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7554\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapplymap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DataFrame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_empty_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;31m# wrap results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    303\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m                     \u001b[0;31m# ignore SettingWithCopy here in case the user mutates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m                     \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m                         \u001b[0;31m# If we have a view on v, we need to make a copy because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-c2feae48d56c>\u001b[0m in \u001b[0;36mdicionario\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m     29\u001b[0m   \u001b[0;31m#dicionario = corpora.Dictionary(row['text_stopwords'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m   \u001b[0mdicionario_bow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdicionario\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text_stopwords'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_update\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdicionario_bow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'doc2bow'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUZGPNhEWl9c"
      },
      "source": [
        "dicionario_bigrama[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TphfH_MvV3Oz"
      },
      "source": [
        "#from gensim.utils import simple_preprocess\r\n",
        "# outra forma de tokenizar documento no gensim, alem de outras funcionalides\r\n",
        "#doc_tokenizado = simple_preprocess(df_movies['text'].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmmhJ9XAXlpp"
      },
      "source": [
        "#doc_tokenizado[0:4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZpbXanKXt95"
      },
      "source": [
        "bow_text = [dicionario_bigrama.doc2bow(line) for line in df_processado['text_stopwords']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLbiWzFqZH2J"
      },
      "source": [
        "bow_text[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HStcc2x3RhFd"
      },
      "source": [
        "# aqui vamos contruir nosso dicionario e ao mesmo tempo colocar o nosso dado no formato bow\r\n",
        "bow_corpus = dicionario_bigrama.doc2bow(df_processado['text_stopwords'], allow_update=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZGP2o3TXfVx"
      },
      "source": [
        "bow_corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bry8oLctPhJ2"
      },
      "source": [
        "# fim testes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oYWoefmX1an"
      },
      "source": [
        "# 2 - Representação\n",
        "\n",
        "Vimos durante a nossa aula diversas forma de representar um documento de texto. Você vai usar cada uma dessas representações e compará-las. A seguir temos a listagem das representações que devem ser usadas para representar seu texto.\n",
        "\n",
        "a) Representação TF-IDF. Você pode usar tanto o gensim quanto o scikit para montar esta representação, mas lembre-se que é importante fazer o pré-processamento dos textos.\n",
        "\n",
        "b) Representação com o word2vec. O modelo poderá ser o apresentado na aula 03 ou algum outro modelo pré-treinado como os existentes no repositório http://nilc.icmc.usp.br/nilc/index.php/repositorio-de-word-embeddings-do-nilc . Neste caso, cada documento deverá ser representado pelo vetor que resultar da média dos vetores de todas as palavras que o compõem. Em outras palavras, se D é composto pelas palavras w1, w2, …, wn, e seus vetores embeddings são v1, v2, …, vn, então a representação do documento de D será v = (v1 + v2 + … + vn) / n.\n",
        "\n",
        "c) Extração de features do texto. Você deve pensar em ao menos 10 features para extrair do documento e que o possam representar. Aqui vão algumas sugestões: número de palavras, número de verbos, número de conjunções, número de palavras negativas, número de palavras fora do vocabulário, quantidades de entidades do tipo PESSOA, quantidade de entidades do tipo LOCAL, etc. \n",
        "\n",
        "Lembrando que você deve dividir seu conjunto em treino e teste. No TF-IDF, você só pode aplicar o método fit no conjunto de treino. Uma sugestão é dividir 80% do conjunto de dados para treino e 20% para teste. Essa divisão é aleatória, mas você pode usar o método train_test_split para essa divisão. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLTaOunLPaSi"
      },
      "source": [
        "# Definindo X e y\r\n",
        "\r\n",
        "X = df_processado['text_stopwords']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Th4iQoxiX1ao"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20 , random_state = 42 )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMW-WQTtX1ap"
      },
      "source": [
        "## Representação TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LX_Yzvymb60"
      },
      "source": [
        "df_processado['text_stopwords'][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXzQBov2kHD8"
      },
      "source": [
        "teste = df_processado['text_stopwords'][0].split(' \" ') \r\n",
        "teste"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C003BSkvX1aq"
      },
      "source": [
        "# cria um dicionário de palavras\r\n",
        "dct = Dictionary(df_processado['text_stopwords'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zW6WWSecX1aq"
      },
      "source": [
        "# converte o corpus para o formato de saco de palavras\n",
        "corpus_filmes_bow = [dct.doc2bow(line) for line in df_processado['text_stopwords']]  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYjX5uXBX1ar"
      },
      "source": [
        "model = TfidfModel(corpus_filmes_bow) # computa para cada termo, a fórmula tf-idf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRzBvN0RX1ar"
      },
      "source": [
        "model[corpus_filmes_bow[230]] # vamos ver uma das representações"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXzvBElpX1as"
      },
      "source": [
        "## Representação WORD2VEC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIA9IZ_IWSix"
      },
      "source": [
        "# o algoritmo word2vec processa as palavras por sentença. Então precisamos dividir os textos\r\n",
        "# como sentenças para alimentar o nosso modelo. A função sent_tokenize do ntlk faz isso.\r\n",
        "corpus_movies = []\r\n",
        "for f_id in df_processado['text_stopwords'].fileids():\r\n",
        "    sent_lst = sent_tokenize(df_processado['text_stopwords'].raw(f_id))\r\n",
        "    for s in sent_lst:\r\n",
        "        corpus_movies.append(simple_preprocess(s))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-s3Os2FX1as"
      },
      "source": [
        "# parametros\n",
        "# min_count = Ignora todas as palavras com frequencia absoluta total\n",
        "# menor que isso\n",
        "# window = A distância máxima entre a palavra corrente e palavra predita em uma sentença. Aqui, duas palavras\n",
        "# palavras anteriores e duas palavras anteriores a palavra corrente.\n",
        "# size = Dimensão dos vetores densos ou word embeddings. \n",
        "w2v_model = Word2Vec(window=2, size=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c79MhOudX1at"
      },
      "source": [
        "# o word2vec demanda a construção de uma tabela representando o vocabulário do nosso corpus.\n",
        "# Ou seja, simplesmente \"digerir\" todas as palavras e filtrar as palavras únicas, e fazer uma\n",
        "# contagem básica delas.\n",
        "w2v_model.build_vocab(X, progress_per=10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zvtyf9_X1at"
      },
      "source": [
        "# parametros\n",
        "# total_examples = Contagem de sentenças\n",
        "# epochs = Número de iterações (épocas) sobre o corpus\n",
        "\n",
        "w2v_model.train(X, total_examples=w2v_model.corpus_count, epochs=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qFSa1WPX1au"
      },
      "source": [
        "## Representação EXTRAÇÃO DE FEATURES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNeOyjRYUiGL"
      },
      "source": [
        "def get_features(data:np.array)->dict:\r\n",
        "    \r\n",
        "    dct_feats = {}\r\n",
        "    dct_feats = preprocess(data, feats=True)\r\n",
        "    \r\n",
        "    nlp = spacy.load('pt')\r\n",
        "\r\n",
        "    # Define o caminho da pasta para o dicionário ortográficos\r\n",
        "    PATH_TO = '/nlp/Projeto/aes-pt-master/dict'\r\n",
        "    # Carrega o dicionário ortográfico\r\n",
        "    hobj = Hunspell('pt_BR','pt_BR.aff', hunspell_data_dir=PATH_TO)\r\n",
        "\r\n",
        "    # Extraindo a quantidade de léxicos enviesados\r\n",
        "    dct_vies = {\r\n",
        "        'argumentativo':[],\r\n",
        "        'pressuposicao':[],\r\n",
        "        'possibilidade_necessidade':[],\r\n",
        "        'opiniao_valoracao':[]}\r\n",
        "\r\n",
        "    # Lê o dicionário de léxicos enviesadas\r\n",
        "    fd_dicionario_vies = open(\"dict/bias_words.txt\",\"r\", encoding='utf-8')\r\n",
        "\r\n",
        "    # Percorre o dicionário classificando cada termo\r\n",
        "    for line in fd_dicionario_vies:\r\n",
        "        entry = line.replace(\"\\n\",\"\").split(\",\")\r\n",
        "        term = entry[0].strip()\r\n",
        "        type_term = entry[1].strip()\r\n",
        "        dct_vies[type_term].append(term)\r\n",
        "\r\n",
        "    # Listas dos tipos de léxicos enviesados\r\n",
        "    arr_arg, arr_press, arr_pos_nec, arr_op_val = [],[],[],[]\r\n",
        "\r\n",
        "    # Listas das quantidades dos tipos de entidades\r\n",
        "    arr_per, arr_loc, arr_org, arr_misc = [],[],[],[]\r\n",
        "\r\n",
        "    # Listas de grupos por tamanho das sentenças\r\n",
        "    arr_sent5, arr_sent5_10, arr_sent10 = [],[],[]\r\n",
        "\r\n",
        "    # Listas das quantidade de caracteres e erros ortográficos\r\n",
        "    arr_erros, arr_chars = [],[]\r\n",
        "\r\n",
        "    # Listas de pos tagging\r\n",
        "    arr_verb, arr_adj, arr_noun, arr_adv = [],[],[],[]\r\n",
        "\r\n",
        "    for i in range(len(data)):\r\n",
        "\r\n",
        "        # Istancia o documento spacy\r\n",
        "        doc = nlp(str(data[i]))\r\n",
        "\r\n",
        "        # Contadores de palavras na sentença\r\n",
        "        qtd_sent5, qtd_sent5_10, qtd_sent10 = 0,0,0\r\n",
        "\r\n",
        "        # Separa o texto pelas sentenças\r\n",
        "        for sent in sent_tokenize(doc.text):\r\n",
        "\r\n",
        "            # Cria o array de tokens excluindo acentuação e stopword\r\n",
        "            arr_tok = [word for word in word_tokenize(sent)\\\r\n",
        "                       if word not in stop_words+[\"``\",\"''\",\"“\",'”']]\r\n",
        "\r\n",
        "            # Verifica o quantidade de tokens na senteça\r\n",
        "            if (len(arr_tok) <= 5):\r\n",
        "                # Conta as sentenças com 5 ou menos tokens\r\n",
        "                qtd_sent5 += 1\r\n",
        "\r\n",
        "            elif (len(arr_tok) > 10):\r\n",
        "                # Conta as sentenças com mais de 10 tokens\r\n",
        "                qtd_sent10 += 1\r\n",
        "            else:\r\n",
        "                # Conta as sentenças com 5 a 10 tokens\r\n",
        "                qtd_sent5_10 += 1            \r\n",
        "\r\n",
        "        arr_sent5.append(qtd_sent5 / len(doc))\r\n",
        "        arr_sent5_10.append(qtd_sent5_10 / len(doc))\r\n",
        "        arr_sent10.append(qtd_sent10 / len(doc))\r\n",
        "\r\n",
        "        # Contadores de cada tipo de entidade\r\n",
        "        qtd_per, qtd_loc, qtd_org, qtd_misc = 0,0,0,0\r\n",
        "        for entidy in doc.ents:\r\n",
        "            if entidy.label_ == 'PER':\r\n",
        "                qtd_per += 1\r\n",
        "            elif entidy.label_ == 'LOC':\r\n",
        "                qtd_loc += 1\r\n",
        "            elif entidy.label_ == 'ORG':\r\n",
        "                qtd_org += 1\r\n",
        "            elif entidy.label_ == 'MISC':\r\n",
        "                qtd_misc += 1\r\n",
        "\r\n",
        "        arr_per.append(qtd_per / len(doc))\r\n",
        "        arr_loc.append(qtd_loc / len(doc))\r\n",
        "        arr_org.append(qtd_org / len(doc))\r\n",
        "        arr_misc.append(qtd_misc / len(doc))\r\n",
        "\r\n",
        "        # Contadores de erros ortográficos e caracteres\r\n",
        "        qtd_erro, qtd_char = 0, 0\r\n",
        "\r\n",
        "        for tok in doc:\r\n",
        "            if tok.text not in stop_words+[\"–\",\"—\"]:\r\n",
        "\r\n",
        "                token = tok.text\r\n",
        "                qtd_char += len(token)\r\n",
        "\r\n",
        "                if not (hobj.spell(token)):\r\n",
        "                    qtd_erro += 1\r\n",
        "\r\n",
        "        arr_erros.append(qtd_erro / len(doc))\r\n",
        "        arr_chars.append(qtd_char / len(doc))\r\n",
        "\r\n",
        "        for type_term in dct_vies:\r\n",
        "            qtd_arg, qtd_pres, qtd_poss, qtd_opin = 0,0,0,0\r\n",
        "            for term in dct_vies[type_term]:\r\n",
        "                for sent in doc.sents:\r\n",
        "                    if term in sent.text.lower():\r\n",
        "                        if type_term == \"argumentativo\":\r\n",
        "                            qtd_arg += 1\r\n",
        "                        elif type_term == \"pressuposicao\":\r\n",
        "                            qtd_pres += 1\r\n",
        "                        elif type_term == \"possibilidade_necessidade\":\r\n",
        "                            qtd_poss += 1\r\n",
        "                        elif type_term == \"opiniao_valoracao\":\r\n",
        "                            qtd_opin += 1\r\n",
        "\r\n",
        "        arr_arg.append(qtd_arg / len(doc))\r\n",
        "        arr_press.append(qtd_pres / len(doc))\r\n",
        "        arr_pos_nec.append(qtd_poss / len(doc))\r\n",
        "        arr_op_val.append(qtd_opin / len(doc))\r\n",
        "\r\n",
        "        qtd_verb, qtd_adj, qtd_noun, qtd_adv = 0,0,0,0\r\n",
        "        for tok in doc:\r\n",
        "            if tok.pos_ == 'VERB':\r\n",
        "                qtd_verb += 1\r\n",
        "            if tok.pos_ == 'ADJ':\r\n",
        "                qtd_adj += 1\r\n",
        "            if tok.pos_ == 'NOUN':\r\n",
        "                qtd_noun += 1\r\n",
        "            if tok.pos_ == 'ADV':\r\n",
        "                qtd_adv += 1\r\n",
        "\r\n",
        "        arr_verb.append(qtd_verb / len(doc))\r\n",
        "        arr_adj.append(qtd_adj / len(doc))\r\n",
        "        arr_noun.append(qtd_noun / len(doc))\r\n",
        "        arr_adv.append(qtd_adv / len(doc))        \r\n",
        "    \r\n",
        "\r\n",
        "    dct_feats[\"sent5\"] = arr_sent5\r\n",
        "    dct_feats[\"sent5_10\"] = arr_sent5_10\r\n",
        "    dct_feats[\"sent10\"] = arr_sent10\r\n",
        "    \r\n",
        "    dct_feats[\"erros\"] = arr_erros\r\n",
        "    dct_feats[\"chars\"] = arr_chars\r\n",
        "\r\n",
        "    dct_feats[\"per\"] = arr_per\r\n",
        "    dct_feats[\"loc\"] = arr_loc\r\n",
        "    dct_feats[\"org\"] = arr_org\r\n",
        "    dct_feats[\"misc\"] = arr_misc\r\n",
        "    \r\n",
        "    dct_feats[\"arg\"] = arr_arg\r\n",
        "    dct_feats[\"press\"] = arr_press\r\n",
        "    dct_feats[\"pos_nec\"] = arr_pos_nec\r\n",
        "    dct_feats[\"op_val\"] = arr_op_val\r\n",
        "    \r\n",
        "    dct_feats[\"verb\"] = arr_verb\r\n",
        "    dct_feats[\"adj\"] = arr_adj\r\n",
        "    dct_feats[\"noun\"] = arr_noun\r\n",
        "    dct_feats[\"adv\"] = arr_adv\r\n",
        "    \r\n",
        "    return dct_feats\r\n",
        "\r\n",
        "dct_fts = get_features(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjQcWCwvoE03"
      },
      "source": [
        "# 3 - Visualização dos dados:\n",
        "Coloque nesta seção os gráficos do PCA e do t-SNE, para cada representação. Responda também às seguintes perguntas: \n",
        "\n",
        "a) Existe algum padrão com relação às classes? \n",
        "\n",
        "b) Caso exista algum padrão, você pode concluir alguma coisa? \n",
        "\n",
        "c) Caso não exista, você consegue dizer se isso tem a ver com alguma representação ou classe?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkyHVCNPOmcl"
      },
      "source": [
        "from sklearn.decomposition import PCA\r\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGyerdbXOI0_"
      },
      "source": [
        "**PCA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqCSm2IeOnwj"
      },
      "source": [
        "from sklearn.decomposition import PCA\r\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iS-QSeRfUpnd"
      },
      "source": [
        "# Voltar com o formato da feature para texto, mas sem as stop words\r\n",
        "nltk.download('machado')\r\n",
        "corpus_filmes = []\r\n",
        "for f_id in machado.fileids():\r\n",
        "    tok_lst = simple_preprocess(df_processado.raw(f_id)) #convertendo para o formato de texto\r\n",
        "    corpus_filmes.append(' '.join(tok_lst)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvA7C7cxU1Ag"
      },
      "source": [
        "df_processado[1:4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dx0WsBFsVZyP"
      },
      "source": [
        "matrix = df_processado['text_stopwords'].fit_transform(corpus_filmes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbeoLKK0Vp60"
      },
      "source": [
        "!pip install -U yellowbrick\r\n",
        "from yellowbrick.features import PCA\r\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjH2lB0TVq04"
      },
      "source": [
        "sc = StandardScaler(with_mean=False)\r\n",
        "matrix = sc.fit_transform(matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06LcT8rfVyqN"
      },
      "source": [
        "# o que podemos concluir? os pontos são muito parecido, mas existem dois bem diferentes? \r\n",
        "# quem são esses dois?\r\n",
        "visualizer = PCA(scale=True)\r\n",
        "visualizer.fit_transform(matrix.toarray())\r\n",
        "visualizer.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhUANEojOK0Q"
      },
      "source": [
        "t-**SNE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxTywK9lN37G"
      },
      "source": [
        "from yellowbrick.text import TSNEVisualizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLtNdnaTV2GR"
      },
      "source": [
        "tsne = TSNEVisualizer()\r\n",
        "\r\n",
        "# se existissem classes, poderiamos visualizar os pontos com cores diferentes. Cada um de acordo\r\n",
        "# com uma classe.\r\n",
        "tsne.fit(matrix.toarray())\r\n",
        "tsne.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJdNwYb9kEDh"
      },
      "source": [
        "# 4 - Classificadores : \n",
        "Descreva sucintamente os dois classificadores escolhidos. Você usou algum parâmetro que não seja padrão? Se sim, mencione nesta seção."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMk19I8JWO0h"
      },
      "source": [
        "#EXEMPLO\r\n",
        "\r\n",
        "A tarefa escolhida foi a classificação de redações no estilo do ENEM. O objetivo é classificar cada redação de acordo com o grupo de \r\n",
        "notas 0, 200, 400, 600, 800 e 1000. O Corpus possui 1840 instâncias e a média de tokens por documento é 226,85.\r\n",
        "print(f'Quantidade de instâncias do Corpus: {len(data)}')\r\n",
        "print(f'Média de tokens por documento: {np.mean(toks)}')\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwMrGsQKkJOR"
      },
      "source": [
        "# 5 - Resultados: \n",
        "Escreva código que execute a validação cruzada em 5-folds para os dois classificadores escolhidos. Também responda às seguintes perguntas: Os embeddings realmente mostraram um resultado melhor que o TF-IDF? Se não, qual foi a representação que teve o melhor desempenho? A diferença foi muito grande?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnqG24ZTkMsu"
      },
      "source": [
        "# 6 - Conclusão: \n",
        "Por fim fale aqui o que você conclui das visualizações e dos resultados. Tente explicar em detalhes por que um resultado, na sua opinião, foi melhor do que outro. Esta explicação pode incluir hipóteses para resultados melhores ou resultados piores. Também pode falar das dificuldades enfrentadas durante o trabalho e como conseguiu contorná-las."
      ]
    }
  ]
}